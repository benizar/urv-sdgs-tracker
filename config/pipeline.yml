# File: config/pipeline.yml
# Global configuration for the URV SDGs tracker pipeline.
#
# This file is meant to centralise configuration for all pipeline phases:
#   - load (data access and loading)
#   - translate (optional translation layer)
#   - sdg_detection (text2sdg parameters, model choices, etc.)
#
# Many sections and fields are commented out for now. They are placeholders
# for future extensions and documentation.

global:
  project_name: "URV SDGs tracker"

  parallel:
    enabled: false

    # NOTE: currently, run_pipeline.R uses future::plan(multisession, ...)
    # regardless of this value. We keep it here as a placeholder for later:
    backend: "multisession"
    workers: 4

# -------------------------------------------------------------------
# load section
# -------------------------------------------------------------------
load:
  # Base directory to store all loaded datasets.
  root: "data"

  # Repository-level folder inside `root`. In practice this should
  # match the top-level folder inside the Zenodo zip.
  dir: "urv_teaching_guides"

  # Subdirectory inside `root/dir` for a specific scraping batch.
  subdir: "scraped-20251127"

  # URL of the Zenodo (or other) archive to download when the load
  # directory does not exist yet.
  url: ""

  # Local path to store the downloaded archive, relative to the project root.
  archive_path: "data/urv_teaching_guides.zip"

  # If TRUE, always re-download the archive even if the local file exists.
  force_download: false

  # If TRUE, the zip is unzipped into `root`. The pipeline assumes
  # the archive creates a folder called `dir` under `root`.
  zip: true

  # Optional format identifier for the scraping layout.
  # format: "web_scraper_v1"

# -------------------------------------------------------------------
# translate section
# -------------------------------------------------------------------
translate:
  enabled: true

  # Translation mode:
  #   - "auto": call the configured translation service and (re)generate CSV files in output_dir
  #   - "reviewer": do NOT call any service; read existing CSV files from output_dir
  #                 (use this after manually reviewing/editing translations)
  mode: "auto"

  # Translation backend used in "auto" mode.
  # Keep consistent with the running services (docker-compose).
  service: "libretranslate"

  # "auto" lets the service detect the source language.
  source_lang: "auto"
  target_lang: "en"

  # Parallelisation settings for translation batches.
  max_cores: 4
  batch_size: 200

  # Folder where per-column translation CSVs are created/read.
  # Files are named: <column>-<target_lang>-<service>.csv
  output_dir: "sandbox/translations"

  # Columns in guides_loaded to translate.
  # Each one will produce an extra column with suffix: _en
  columns:
    - course_description
    - course_contents
    - course_competences_and_results
    - course_references

  # If TRUE, perform a service healthcheck before translating (auto mode).
  # In reviewer mode, this check is skipped by the code.
  check_service: true

  # Service endpoints (used by translation helpers + healthcheck).
  libretranslate_base_url: "http://libretranslate:5000"
  apertium_base_url: "http://apertium:2737"

  # Healthcheck details (only used when check_service is true).
  healthcheck_path: "/health"
  healthcheck_timeout_sec: 60
  healthcheck_poll_interval_sec: 2

# -------------------------------------------------------------------
# sdg_detection section
# -------------------------------------------------------------------
sdg_detection:
  # Detection method:
  #   - "systems": run specified systems and aggregate hits
  #   - "ensemble": use text2sdg ensemble detector (single output, no per-system split)
  method: "systems"

  # Systems supported by text2sdg (names must match what the package expects).
  systems:
    - "Aurora"
    - "Elsevier"
    - "Auckland"
    - "SIRIS"
    - "SDGO"
    - "SDSN"

  # SDG ids to evaluate.
  sdgs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]

  verbose: false

  # Groups of columns to concatenate into "sections" for detection.
  # Each group becomes a section id (sdg_input$section) and a text string:
  #   paste0(prefix, <col1>, "\n", <col2>, ...)
  combine_groups:
    - id: "course_info"
      prefix: "Course information: "
      columns:
        - course_description_en
        - course_contents_en
    - id: "competences"
      prefix: "Competences and learning outcomes: "
      columns:
        - course_competences_and_results_en
    - id: "references"
      prefix: "References: "
      columns:
        - course_references_en

  aggregate:
    # Aggregation strategy for the final SDG signal.
    # "counts" typically means counting hits per SDG (and applying min_hits).
    mode: "counts"
    min_hits: 1
